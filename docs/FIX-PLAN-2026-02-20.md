# Fix Plan: Remaining Slack Routing Bugs

**Date:** 2026-02-20
**Status:** In Progress (Fixes 0, 1, 2 implemented)
**Prerequisite:** Recent commits (47 on Gateway, ~20 on ClawTalk) already fixed triple-path processing and agent routing conflicts.

---

## Critical Root Cause Discovery

The `message_received` hook in OpenClaw is **fire-and-forget** (`runVoidHook`). The `{ cancel: true }` return from `handleSlackMessageReceivedHook()` is **silently discarded** by OpenClaw. This means the hook CANNOT prevent OpenClaw from processing Slack messages.

The actual gating mechanism is `maybeHandoffSlackInbound()` in OpenClaw's `src/slack/monitor/handoff.ts`, which calls ClawTalkGateway's `/api/events/slack` endpoint BEFORE processing. However, this handoff is only active when `CLAWTALK_SLACK_HANDOFF_ENABLED=1` is set or the config flag `gateway.clawtalk.slackHandoff.enabled` is true in `openclaw.json`.

**Without the handoff enabled, OpenClaw processes every Slack message natively AND ClawTalk also processes it via the hook — causing duplicates and "No response" errors.**

---

## Bug Summary (Post-Pull)

| # | Bug | Severity | File(s) | Status |
|---|-----|----------|---------|--------|
| 0 | Handoff not auto-configured | **Critical** | `slack-routing-sync.ts`, `handoff.ts` | **Fixed** |
| 1 | Outbound suppression TTL leak | **High** | `slack-ingress.ts` | **Fixed** |
| 2 | Dead letter race condition | **High** | `slack-ingress.ts` | **Fixed** |
| 3 | Ownership doctor detection-only | **Medium** | `slack-ownership-doctor.ts`, `index.ts` | Open |
| 4 | Slack routing sync only at startup | **Medium** | `slack-routing-sync.ts`, `index.ts` | Open |
| 5 | Gateway origin resolution inconsistency | **Low** | `index.ts`, `slack-ingress.ts`, `event-dispatcher.ts` | Open |

---

## Fix 0: Auto-Configure Slack Handoff (Critical — IMPLEMENTED)

### Problem

The handoff endpoint (`maybeHandoffSlackInbound`) is the ONLY way to prevent OpenClaw from processing Slack messages that ClawTalk should own. But it requires `CLAWTALK_SLACK_HANDOFF_ENABLED=1` or a config entry, which was never set automatically.

### Implementation

**ClawTalkGateway (`slack-routing-sync.ts`):**
- Added `talksNeedSlackHandoff()` — scans Talks to determine if handoff is needed based on:
  - `full_control` execution mode (always needs handoff)
  - Custom `platformBehaviors` on Slack bindings
  - Tool allow/deny lists
  - Vanilla `openclaw` mode with no custom behaviors → handoff NOT needed
- Added `reconcileSlackHandoffConfig()` — writes `gateway.clawtalk.slackHandoff.enabled` and `.endpoint` into `openclaw.json`
- Called during existing `reconcileSlackRoutingForTalks()` at startup

**OpenClaw (`handoff.ts`):**
- Updated `resolveHandoffEndpoint()` to check config in addition to env vars
- Priority: env var URL > env var toggle > config file flag
- Added `resolveHandoffFromConfig()` to read `gateway.clawtalk.slackHandoff` from config

---

## Fix 1: Outbound Suppression TTL Leak (High Priority)

### Problem

When ClawTalk claims a Slack message via the handoff endpoint, it places an outbound suppression lease to prevent OpenClaw from also replying. The lease has two limits:

- **Time limit:** `DEFAULT_SUPPRESS_TTL_MS = 120_000` (120 seconds)
- **Cancel count limit:** `DEFAULT_SUPPRESS_MAX_CANCELS = 3`

If the LLM + tool-loop call takes longer than 120s (common with complex tool chains), or OpenClaw attempts to send more than 3 messages for the same conversation, the suppression expires and OpenClaw's reply leaks through — causing **duplicate messages** in Slack.

### Root Cause

The suppression is created once at ownership-decision time (`upsertOutboundSuppression` at line ~2167 of `slack-ingress.ts`) with a fixed TTL. It is never refreshed during processing, even though the queue worker may take minutes to complete.

### Proposed Fix

**A) Extend suppression during active processing**

In `processQueue()` / `processQueueItem()`, refresh the suppression lease periodically while the item is being actively processed:

```typescript
// In the processQueueItem function, before and during the LLM call:
function refreshSuppression(item: QueueItem): void {
  upsertOutboundSuppression(item.event, item.talkId);
}

// Before LLM call
refreshSuppression(item);

// After LLM call completes (before send)
refreshSuppression(item);
```

**B) Add a heartbeat interval during tool-loop execution**

For long-running tool loops, add a periodic refresh:

```typescript
const suppressionRefreshInterval = setInterval(() => {
  upsertOutboundSuppression(item.event, item.talkId);
}, 30_000); // Refresh every 30s
suppressionRefreshInterval.unref();

try {
  const llmResult = await callLlmForEvent({ ... });
  // ...
} finally {
  clearInterval(suppressionRefreshInterval);
}
```

**C) Increase default cancel count**

Change `DEFAULT_SUPPRESS_MAX_CANCELS` from 3 to 10. Tool-loop interactions can generate multiple intermediate OpenClaw messages (e.g., tool results, status updates).

### Files to Modify

- `src/slack-ingress.ts`:
  - Add `refreshSuppression()` helper
  - Call it in `processQueueItem()` before LLM call, after LLM call, and on retry
  - Add heartbeat interval during tool-loop execution
  - Change `DEFAULT_SUPPRESS_MAX_CANCELS` from 3 to 10

### Verification

- Test with a Talk that has tool-loop enabled and a tool that takes >120s
- Verify no duplicate Slack messages appear
- Check suppression debug logs show refresh events

---

## Fix 2: Dead Letter Race Condition (High Priority)

### Problem

If a suppression expires mid-processing (despite Fix 1's heartbeat), there's no way to detect that OpenClaw has already replied. ClawTalk will still send its response, resulting in a duplicate.

Additionally, if the LLM call fails after suppression has already blocked one or more OpenClaw outbound messages, the user gets **no response at all** — a "dead letter."

### Root Cause

The suppression system is fire-and-forget: once placed, there's no feedback loop between the suppression consumer (`handleSlackMessageSendingHook`) and the queue worker.

### Proposed Fix

**A) Track suppression consumption in the queue item**

Add a callback/counter so the queue worker knows how many OpenClaw messages were suppressed:

```typescript
type OutboundSuppressionLease = {
  // ... existing fields ...
  consumedCount: number;  // NEW: track how many outbound messages were suppressed
};
```

**B) Check suppression state before sending ClawTalk reply**

Before `sendSlackReply()`, check if the suppression has expired or been fully consumed. If it has, log a warning but still attempt to send (since OpenClaw's reply may have been incomplete or the suppression may have blocked partial messages):

```typescript
// In processQueueItem, before sendSlackReply:
const suppressionKey = buildSuppressionKey(item.event.accountId, normalizeTarget(item.event.outboundTarget));
const activeSuppression = outboundSuppressions.get(suppressionKey);
if (!activeSuppression) {
  deps.logger.warn(
    `SlackIngress: suppression expired before reply send for event=${item.event.eventId} ` +
    `— possible duplicate message`
  );
  // Still send, but emit a diagnostic
}
```

**C) Dead letter recovery: send failure notice**

If LLM call fails AND suppressions were consumed (meaning OpenClaw messages were blocked), send a brief failure notice to the Slack channel so the user isn't left in the dark:

```typescript
// In the retry-exhausted error handler:
if (suppressionConsumedForEvent(item.event.eventId)) {
  await sendSlackReply({
    deps,
    event: item.event,
    talkId: item.talkId,
    accountId: item.replyAccountId,
    message: '⚠️ I encountered an error processing this message. Please try again.',
    sessionKey: item.sessionKey ?? 'error-recovery',
    deliveryMode: item.behaviorDeliveryMode,
    intent: item.behaviorIntent,
  });
}
```

### Files to Modify

- `src/slack-ingress.ts`:
  - Add `consumedCount` to `OutboundSuppressionLease`
  - Increment `consumedCount` in `consumeOutboundSuppression()`
  - Add suppression-state check before `sendSlackReply()`
  - Add dead letter recovery in the error handler

### Verification

- Simulate an LLM timeout (set `CLAWTALK_INGRESS_LLM_TIMEOUT_MS=5000`) with a Slack binding
- Verify that either: the reply is sent (suppression still active), or a failure notice appears
- No silent drops

---

## Fix 3: Ownership Doctor Auto-Remediation (Medium Priority)

### Problem

`findOpenClawSlackOwnershipConflicts()` detects when Talk Slack bindings overlap with non-ClawTalk OpenClaw agent bindings, but only logs warnings. The conflict causes OpenClaw to route messages to its own agent instead of letting ClawTalk handle them.

### Root Cause

The ownership doctor was designed as a diagnostic tool, not a fixer. The `index.ts` wiring calls it on startup and logs conflicts, but takes no corrective action.

### Proposed Fix

**A) Add remediation function to `slack-ownership-doctor.ts`**

```typescript
export async function remediateSlackOwnershipConflicts(params: {
  conflicts: SlackOwnershipConflict[];
  openClawConfigPath: string;
  logger: Logger;
}): Promise<{ fixed: number; errors: string[] }> {
  // For each conflict:
  // 1. Read openclaw.json
  // 2. Find the conflicting binding by agentId + scope
  // 3. Either:
  //    a) Remove the binding if it's fully redundant with a Talk binding
  //    b) OR reorder it below ClawTalk's binding so ClawTalk wins
  // 4. Write back
}
```

**B) Wire remediation into startup**

In `index.ts`, after detecting conflicts, call the remediation function:

```typescript
// After findOpenClawSlackOwnershipConflicts:
if (conflicts.length > 0) {
  api.logger.warn(`ClawTalk: ${conflicts.length} Slack ownership conflict(s) detected`);
  const result = await remediateSlackOwnershipConflicts({
    conflicts,
    openClawConfigPath: path.join(home, '.openclaw', 'openclaw.json'),
    logger: api.logger,
  });
  if (result.fixed > 0) {
    api.logger.info(`ClawTalk: remediated ${result.fixed} ownership conflict(s)`);
  }
}
```

**C) Add a diagnostic issue for unresolvable conflicts**

If remediation fails (e.g., filesystem permissions), surface it as a `TalkDiagnosticIssue` so the ClawTalk client can display it.

### Files to Modify

- `src/slack-ownership-doctor.ts`: Add `remediateSlackOwnershipConflicts()`
- `src/index.ts`: Wire remediation into startup and `gateway_start` hook

### Risk

- Modifying `openclaw.json` could break other agents. Must be careful to only reorder/remove bindings, not add new ones.
- Should write a backup of the config before modifying.

### Verification

- Create a Talk with a Slack binding that overlaps with an existing OpenClaw agent binding
- Restart gateway
- Verify the conflict is resolved and ClawTalk owns the channel

---

## Fix 4: Slack Routing Sync on Binding Change (Medium Priority)

### Problem

`reconcileSlackRoutingForTalks()` only runs on `gateway_start`. When a user creates or modifies a Talk's Slack platform binding via the API, the change isn't synced to OpenClaw's config until the gateway restarts.

This means new Talk Slack bindings don't take effect until restart.

### Root Cause

No hook or watcher triggers `reconcileSlackRoutingForTalks()` when Talk metadata changes.

### Proposed Fix

**A) Trigger sync on Talk metadata changes**

In `talks.ts`, after any PATCH to a Talk's `platformBindings`, call the sync function:

```typescript
// In the PATCH /api/talks/:id handler, after saving:
if (changedFields.includes('platformBindings')) {
  reconcileSlackRoutingForTalks({
    store,
    logger,
    openClawConfigPath: path.join(home, '.openclaw', 'openclaw.json'),
  }).catch(err => {
    logger.warn(`ClawTalk: post-binding-change sync failed: ${err}`);
  });
}
```

**B) Debounce rapid changes**

If the user makes multiple binding changes quickly (e.g., adding several bindings), debounce the sync to avoid excessive config rewrites:

```typescript
let pendingSyncTimeout: NodeJS.Timeout | null = null;

function scheduleSyncAfterBindingChange(deps: SyncDeps): void {
  if (pendingSyncTimeout) clearTimeout(pendingSyncTimeout);
  pendingSyncTimeout = setTimeout(() => {
    pendingSyncTimeout = null;
    reconcileSlackRoutingForTalks(deps).catch(err => {
      deps.logger.warn(`ClawTalk: debounced sync failed: ${err}`);
    });
  }, 2_000); // 2s debounce
}
```

### Files to Modify

- `src/talks.ts`: Trigger sync after `platformBindings` changes in PATCH handler
- `src/slack-routing-sync.ts`: Add debounced wrapper
- `src/index.ts`: Pass sync dependencies to talk route handlers

### Verification

- Create a new Talk with a Slack binding via the API
- Without restarting, verify the binding appears in OpenClaw's config
- Send a Slack message to the bound channel and verify ClawTalk handles it

---

## Fix 5: Gateway Origin Resolution Consistency (Low Priority)

### Problem

The gateway origin (the base URL for internal HTTP calls to OpenClaw) is resolved differently across files:

- `index.ts`: Computed from `config.gateway` settings with various fallbacks
- `slack-ingress.ts`: Received as `deps.gatewayOrigin`
- `event-dispatcher.ts`: Received as `opts.gatewayOrigin`
- `talk-chat.ts`: Received via handler context

Most paths converge on the same value, but edge cases (Tailscale vs. localhost, IPv4 vs. IPv6) can cause mismatches.

### Proposed Fix

**A) Centralize gateway origin resolution**

Extract the resolution logic into a shared utility:

```typescript
// src/gateway-origin.ts
export function resolveGatewayOrigin(config: Record<string, any>): string {
  // Single source of truth for gateway origin resolution
  const gateway = config.gateway ?? {};
  const port = gateway.port ?? 18789;
  const bind = gateway.bind ?? '127.0.0.1';
  // Always use localhost for internal calls
  return `http://127.0.0.1:${port}`;
}
```

**B) Use the centralized resolver everywhere**

Replace all ad-hoc gateway origin resolution with calls to `resolveGatewayOrigin()`.

### Files to Modify

- New file: `src/gateway-origin.ts`
- `src/index.ts`: Use centralized resolver
- Pass resolved origin consistently to all deps

### Risk

Low. This is a refactor that shouldn't change behavior if the resolution logic is preserved.

---

## Implementation Order

1. **Fix 1** (Suppression TTL refresh) — Highest impact, directly prevents duplicate messages
2. **Fix 2** (Dead letter recovery) — Prevents silent message drops
3. **Fix 4** (Sync on binding change) — Improves UX, no restart needed for binding changes
4. **Fix 3** (Ownership doctor remediation) — Prevents config-level routing conflicts
5. **Fix 5** (Gateway origin) — Low priority refactor

**Estimated scope:** Fixes 1+2 are ~150 lines of changes in `slack-ingress.ts`. Fix 3 is ~80 lines across two files. Fix 4 is ~40 lines. Fix 5 is ~60 lines in a new file + minor changes.

---

## Previously Fixed Issues (Reference)

These were identified in the [original audit](./SLACK-ROUTING-AUDIT-2026-02-20.md) and are now resolved:

| Issue | Fix | Commit Evidence |
|-------|-----|-----------------|
| Triple-path processing (handoff + hook fires twice) | `seenEvents` Map deduplication with 6h TTL | `routeSlackIngressEvent` shared between HTTP handler and hook |
| OpenClaw agent routing conflicts with Talk agents | `assertRoutingHeaders()` blocks forbidden headers in `full_control` mode | `routing-headers.ts` new file |
| Event dispatcher binding match field mismatch | Confirmed working correctly — `ctx.channelId` from OpenClaw IS the platform name | N/A (not a bug) |
